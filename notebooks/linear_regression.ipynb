{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Mmmyayp7s0L"
      },
      "source": [
        "# Linear Regression \n",
        "\n",
        "Linear regression is a statistical method used to model the relationship between a dependent variable (often denoted as \"y\") and one or more independent variables (often denoted as \"x\"). The basic idea of linear regression is to find the straight line that best fits the data points in a scatter plot.\n",
        "\n",
        "The most common form of linear regression is simple linear regression, which models the relationship between two variables:\n",
        "\n",
        "$\\hat{y} = mx + b$\n",
        "\n",
        "where $\\hat{y}$ is the predicted value, $x$ is the independent variable, $m$ is the slope, and $b$ is the intercept. \n",
        "\n",
        "Given a set of input data ($\\{x_i, y_i\\}$), the goal of linear regression is to find the values of $m$ and $b$ that best fit the data\n",
        "\n",
        "\n",
        "The values of $m$ and $b$ are chosen to minimize the ***sum of squared errors***.\n",
        "\n",
        "$SSE = \\sum (y_i - \\hat{y_i})^2$\n",
        "\n",
        "substituting $\\hat{y}_i = m x_i + b$:\n",
        "\n",
        "$$\n",
        "SSE = \\sum (y_i - (m x_i + b))^2\n",
        "$$\n",
        "\n",
        "\n",
        "To minimize $SSE$, we take the partial derivatives with respect to $m$ and $b$ and set them equal to 0.\n",
        "\n",
        "### **Derivative with Respect to $b$:**\n",
        "\n",
        "$$\n",
        "\\frac{\\partial SSE}{\\partial b} = \\sum 2 (y_i - (m x_i + b)) (-1) = -2 \\sum (y_i - m x_i - b) = 0\n",
        "$$\n",
        "\n",
        "Dividing by $-2$:\n",
        "\n",
        "$$\n",
        "\\sum (y_i - m x_i - b) = 0\n",
        "$$\n",
        "\n",
        "Rearranging:\n",
        "\n",
        "$$\n",
        "\\sum y_i = m \\sum x_i + b \\sum 1\n",
        "$$\n",
        "\n",
        "Since $\\sum 1 = n$, we get:\n",
        "\n",
        "$$\n",
        "\\sum y_i = m \\sum x_i + n b\n",
        "$$\n",
        "\n",
        "Dividing by $n$:\n",
        "\n",
        "$$\n",
        "b = \\frac{\\sum y_i}{n} - m \\frac{\\sum x_i}{n}\n",
        "$$\n",
        "\n",
        "Since $x_{\\text{mean}} = \\frac{\\sum x_i}{n}$ and $y_{\\text{mean}} = \\frac{\\sum y_i}{n}$, this simplifies to:\n",
        "\n",
        "$$\n",
        "b = y_{\\text{mean}} - m x_{\\text{mean}}\n",
        "$$\n",
        "\n",
        "### **Derivative with Respect to $m$:**\n",
        "\n",
        "$$\n",
        "\\frac{\\partial SSE}{\\partial m} = \\sum 2 (y_i - (m x_i + b)) (-x_i) = -2 \\sum x_i (y_i - m x_i - b) = 0\n",
        "$$\n",
        "\n",
        "Dividing by $-2$:\n",
        "\n",
        "$$\n",
        "\\sum x_i (y_i - m x_i - b) = 0\n",
        "$$\n",
        "\n",
        "Expanding:\n",
        "\n",
        "$$\n",
        "\\sum x_i y_i - m \\sum x_i^2 - b \\sum x_i = 0\n",
        "$$\n",
        "\n",
        "Substituting $b = y_{\\text{mean}} - m x_{\\text{mean}}$:\n",
        "\n",
        "$$\n",
        "\\sum x_i y_i - m \\sum x_i^2 - (y_{\\text{mean}} - m x_{\\text{mean}}) \\sum x_i = 0\n",
        "$$\n",
        "\n",
        "Rearranging:\n",
        "\n",
        "$$\n",
        "\\sum x_i y_i - y_{\\text{mean}} \\sum x_i = m \\left( \\sum x_i^2 - x_{\\text{mean}} \\sum x_i \\right)\n",
        "$$\n",
        "\n",
        "Dividing by $\\sum x_i^2 - x_{\\text{mean}} \\sum x_i$:\n",
        "\n",
        "$$\n",
        "m = \\frac{\\sum x_i y_i - y_{\\text{mean}} \\sum x_i}{\\sum x_i^2 - x_{\\text{mean}} \\sum x_i}\n",
        "$$\n",
        "\n",
        "Using the mean-centered formulation:\n",
        "\n",
        "$$\n",
        "m = \\frac{\\sum (x_i - x_{\\text{mean}})(y_i - y_{\\text{mean}})}{\\sum (x_i - x_{\\text{mean}})^2}\n",
        "$$\n",
        "\n",
        "Thus, the best-fit parameters for simple linear regression are:\n",
        "\n",
        "$$\n",
        "\\boxed{m = \\frac{\\sum (x_i - x_{\\text{mean}})(y_i - y_{\\text{mean}})}{\\sum (x_i - x_{\\text{mean}})^2}}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\boxed{b = y_{\\text{mean}} - m x_{\\text{mean}}}\n",
        "$$\n",
        "\n",
        "Multiple linear regression is a more general form of linear regression that models the relationship between multiple independent variables and one dependent variable. The formula for the best-fit hyperplane in multiple linear regression is:\n",
        "\n",
        "$$\n",
        "y = w_0 + w_1 x_1 + w_2 x_2 + \\dots + w_n x_n\n",
        "$$\n",
        "\n",
        "where:\n",
        "\n",
        "- $Y$ is the dependent variable (output).\n",
        "- $x_1, x_2, \\dots, x_n$ are the independent variables (input features).\n",
        "- $w_0$ is the **intercept** (bias term).\n",
        "- $w_1, w_2, \\dots, w_n$ are the **weights (coefficients)** that determine the influence of each feature.\n",
        "\n",
        "or, in vectorized form:\n",
        "\n",
        "$$\n",
        "Y = X W\n",
        "$$\n",
        "\n",
        "where:\n",
        "- $Y$ is an  $m \\times 1$ vector (outputs for all data points),\n",
        "- $X$ is an  $m \\times (n+1)$ **design matrix** (including the intercept column),\n",
        "- $W$ is an  $(n+1) \\times 1$ vector of weights.\n",
        "\n",
        "- $X$ is typically represented as:\n",
        "\n",
        "  $$\n",
        "  X =\n",
        "  \\begin{bmatrix}\n",
        "  1 & x_{11} & x_{12} & \\dots & x_{1n} \\\\\n",
        "  1 & x_{21} & x_{22} & \\dots & x_{2n} \\\\\n",
        "  \\vdots & \\vdots & \\vdots & \\dots & \\vdots \\\\\n",
        "  1 & x_{m1} & x_{m2} & \\dots & x_{mn}\n",
        "  \\end{bmatrix}\n",
        "  $$\n",
        "\n",
        "  where each row represents a data sample, and the first column of ones accounts for the intercept term $w_0$.\n",
        "\n",
        "- $W$ is the **weight vector**:\n",
        "\n",
        "  $$\n",
        "  W =\n",
        "  \\begin{bmatrix}\n",
        "  w_0 \\\\\n",
        "  w_1 \\\\\n",
        "  w_2 \\\\\n",
        "  \\vdots \\\\\n",
        "  w_n\n",
        "  \\end{bmatrix}\n",
        "  $$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Code \n",
        "### Simple linear regression \n",
        "Here is a basic implementation of simple linear regression in Python using the least squares method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "aFxnnxGf7u_m"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "class LinearRegression:\n",
        "    def __init__(self):\n",
        "        self.slope = None  # m\n",
        "        self.intercept = None  # b\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n = X.shape[0]  # or we can also take len(X)\n",
        "        x_mean = np.mean(X)\n",
        "        y_mean = np.mean(y)\n",
        "\n",
        "        num = 0\n",
        "        den = 0\n",
        "\n",
        "        for i in range(n):\n",
        "            num = num + ((X[i] - x_mean) * (y[i] - y_mean))\n",
        "            den = den + (X[i] - x_mean) ** 2\n",
        "\n",
        "        self.slope = num / den\n",
        "        self.intercept = y_mean - (self.slope * x_mean)\n",
        "\n",
        "    def predict(self, X):\n",
        "        y_pred = []\n",
        "\n",
        "        for x in X:\n",
        "            y_pred.append(self.slope * x + self.intercept)\n",
        "        return y_pred\n",
        "        # return np.array(y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V7xztH2I-Uu7",
        "outputId": "34b75e6a-cdad-485f-e7ab-c6265fe8137e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Slope (m) =  0.6\n",
            "Intercept (b) =  2.2\n",
            "y_pred = [2.8000000000000003, 3.4000000000000004, 4.0, 4.6, 5.2]\n"
          ]
        }
      ],
      "source": [
        "X = np.array([1, 2, 3, 4, 5])\n",
        "y = np.array([2, 4, 5, 4, 5])\n",
        "lr = LinearRegression()\n",
        "lr.fit(X, y)\n",
        "print(\"Slope (m) = \", lr.slope)\n",
        "print(\"Intercept (b) = \", lr.intercept)\n",
        "y_pred = lr.predict(X)\n",
        "print(\"y_pred =\", y_pred)\n",
        "# print(y_pred.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5UiBg5p5Eq3p"
      },
      "source": [
        "### Derivation of the Vectorized Linear Regression Formula\n",
        "\n",
        "To derive the closed-form solution for the weights W in linear regression using the least squares method.\n",
        "\n",
        "#### **1. Linear Regression Model**\n",
        "The linear regression model can be written in a vectorized form as:\n",
        "\n",
        "$$\n",
        "y = XW\n",
        "$$\n",
        "\n",
        "where:\n",
        "- X is an $(n \\times d)$ matrix (with  n samples and  d features),\n",
        "- W is a $(d \\times 1)$ column vector of model parameters (weights),\n",
        "- y is an $(n \\times 1)$ column vector of target values.\n",
        "\n",
        "#### **2. Loss Function (Least Squares Error)**\n",
        "We use the mean squared error (MSE) loss function:\n",
        "\n",
        "$$\n",
        "L(W) = \\sum_{i=1}^{n} (y_i - X_i W)^2\n",
        "$$\n",
        "\n",
        "which can be written in matrix form as:\n",
        "\n",
        "$$\n",
        "L(W) = (y - XW)^T (y - XW)\n",
        "$$\n",
        "\n",
        "Expanding this:\n",
        "\n",
        "$$\n",
        "L(W) = y^T y - 2 W^T X^T y + W^T X^T X W\n",
        "$$\n",
        "\n",
        "#### **3. Compute the Gradient**\n",
        "To minimize L(W), we take the derivative with respect to  W:\n",
        "\n",
        "$$\n",
        "\\frac{dL}{dW} = -2X^T y + 2X^T X W\n",
        "$$\n",
        "\n",
        "Setting the gradient to zero:\n",
        "\n",
        "$$\n",
        "-2X^T y + 2X^T X W = 0\n",
        "$$\n",
        "\n",
        "$$\n",
        "X^T X W = X^T y\n",
        "$$\n",
        "\n",
        "#### **4. Solve for W**\n",
        "If $X^T X$ is invertible, we multiply both sides by $(X^T X)^{-1}$:\n",
        "\n",
        "$$\n",
        "W = (X^T X)^{-1} X^T y\n",
        "$$\n",
        "\n",
        "#### **Final Vectorized Solution**\n",
        "$$\n",
        "\\boxed{W = (X^T X)^{-1} X^T y}\n",
        "$$\n",
        "\n",
        "This is the closed-form solution for the optimal weights in ordinary least squares (OLS) regression.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "CUfBYJlJJe6Z"
      },
      "outputs": [],
      "source": [
        "class LinearRegression:\n",
        "    def __init__(self):\n",
        "        self.W = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n = X.shape[0]\n",
        "        # Add bias term using column wise concatination X -> [1 X]\n",
        "        X = np.c_[np.ones(n), X]\n",
        "        self.W = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\n",
        "\n",
        "    def predict(self, X):\n",
        "        # Add bias term using column wise concatination\n",
        "        X = np.c_[np.ones(X.shape[0]), X]\n",
        "        y_pred = X.dot(self.W)\n",
        "        return y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UXoq2cfrK-cr",
        "outputId": "a0bdd92f-273f-446d-c1c5-59ef390c6e07"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "W = [3. 1. 2.]\n",
            "y_pred = [ 9. 17. 26.]\n",
            "new_y_pred = [35. 44.]\n"
          ]
        }
      ],
      "source": [
        "# Create example input data\n",
        "X = np.array([[2, 2], [4, 5], [7, 8]])\n",
        "y = np.array([9, 17, 26])\n",
        "\n",
        "lr = LinearRegression()\n",
        "lr.fit(X, y)\n",
        "print(\"W =\", lr.W)\n",
        "y_pred = lr.predict(X)\n",
        "print(\"y_pred =\", y_pred)\n",
        "\n",
        "X_new = np.array([[10, 11], [13, 14]])\n",
        "y_pred = lr.predict(X_new)\n",
        "print(\"new_y_pred =\", y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBvmQ2Zdj22K"
      },
      "source": [
        "### Improvements \n",
        "here are some improvements to the simple linear regression implementation to make it more robust:\n",
        "\n",
        "1. Add input **validation**: Add input validation to check that the input arrays $X$ and $y$ have the same length and are not empty.\n",
        "\n",
        "2. Use NumPy broadcasting: Instead of looping through the data to calculate the numerator and denominator, we can use NumPy broadcasting to perform the calculations in a vectorized way. This will make the code faster and more efficient.\n",
        "\n",
        "3. Add **regularization**: Regularization can help prevent overfitting by adding a penalty term to the cost function. One common regularization technique is L2 regularization, which adds the sum of squares of the coefficients to the cost function. This can be easily added to the code by adding a regularization parameter to the constructor.\n",
        "\n",
        "4. Use **gradient descent**: For large datasets, calculating the inverse of the matrix in the normal equation can be computationally expensive. To overcome this, we can use gradient descent to minimize the cost function. This can be implemented by adding a method that updates the coefficients iteratively using the gradient descent algorithm.\n",
        "\n",
        "Here's the updated code that incorporates these improvements:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "i2HnVgAYLhju"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "class LinearRegressionGD:\n",
        "    def __init__(self, regularization=0):\n",
        "        self.regularization = regularization\n",
        "        self.W = None\n",
        "\n",
        "    def fit(self, X, y, lr=0.01, epochs=1000):\n",
        "        # X: n x d\n",
        "        # Add bias term to X using column wise concatination\n",
        "        X = np.c_[np.ones(X.shape[0]), X]\n",
        "\n",
        "        # Take W with all zeros in d dimensions\n",
        "        self.W = np.zeros(X.shape[1])\n",
        "\n",
        "        for i in range(epochs):\n",
        "\n",
        "            # cal the predicted y value\n",
        "            y_pred = np.dot(X, self.W)\n",
        "\n",
        "            # cal the cost function using ridge regression loss function\n",
        "            cost = np.sum(y_pred - y) ** 2 + self.regularization * np.sum(self.W) ** 2\n",
        "\n",
        "            # cal gradient descent\n",
        "            gradient = 2 * np.dot(X.T, (y_pred - y)) + 2 * self.regularization * self.W\n",
        "\n",
        "            # update W\n",
        "            self.W = self.W - lr * gradient\n",
        "\n",
        "            if i % 1000 == 0:\n",
        "                print(\"cost after 1000 epochs =\", cost)\n",
        "\n",
        "    def predict(self, X):\n",
        "        # Add bias term to X\n",
        "        X = np.c_[np.ones(X.shape[0]), X]\n",
        "        y_pred = np.dot(X, self.W)\n",
        "        return y_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vOqQ4tTWnXBr",
        "outputId": "f28e6723-e8cb-49bf-8109-5082985ce396"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cost after 1000 epochs = 400.0\n",
            "cost after 1000 epochs = 0.7438784376655634\n",
            "cost after 1000 epochs = 0.7438784380617114\n",
            "cost after 1000 epochs = 0.7438784380617114\n",
            "cost after 1000 epochs = 0.7438784380617114\n",
            "cost after 1000 epochs = 0.7438784380617114\n",
            "cost after 1000 epochs = 0.7438784380617114\n",
            "cost after 1000 epochs = 0.7438784380617114\n",
            "cost after 1000 epochs = 0.7438784380617114\n",
            "cost after 1000 epochs = 0.7438784380617114\n",
            "W = [1.99964292 0.65345474]\n",
            "y_pred = [2.65309766 3.3065524  3.96000714 4.61346188 5.26691662]\n"
          ]
        }
      ],
      "source": [
        "X = np.array([[1, 2, 3, 4, 5]]).T\n",
        "y = np.array([2, 4, 5, 4, 5])\n",
        "lr = LinearRegressionGD(regularization=0.1)\n",
        "lr.fit(X, y, lr=0.01, epochs=10000)\n",
        "print(\"W =\", lr.W)\n",
        "y_pred = lr.predict(X)\n",
        "print(\"y_pred =\", y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbxtJcGEsMVT"
      },
      "source": [
        "The model is converging:\n",
        "- The cost function initially decreases rapidly, indicating that the gradient descent is working.\n",
        "- After some point, the cost stops decreasing, which suggests that the model has reached a minimum (either global or local).\n",
        "\n",
        "Gradient updates are very small:\n",
        "- The last few values are almost identical, meaning the weight updates are too small to change the cost.\n",
        "- This usually happens when the model has learned well or reached numerical precision limits.\n",
        "\n",
        "Possible early stopping point:\n",
        "- Since the cost is no longer improving, continuing training won’t help.\n",
        "- You could add an early stopping condition (e.g., stop training if the cost changes by less than a small threshold like 10−610−6)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GoQzNPhzsuWW"
      },
      "source": [
        "Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "ImJLoV9poFTc",
        "outputId": "5e5984a3-d9e6-437d-99a5-7984d1e0a533"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.scatter(X, y, color=\"red\")\n",
        "plt.plot(X, y_pred, color=\"blue\")\n",
        "plt.xlabel(\"X\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.title(\"Linear Regression\")\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
